---
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
```

## Question 6


```{r include=TRUE}
data <- read.csv('BodyFatPercentage.csv')
data <- filter(data,BODYFAT!=0)
(lm <- summary(lm(BODYFAT~WRIST,data=data)))
```

95% confidence intervals for $B_0$ is (`r lm$coefficients[1,1]-(qt(0.975,df=249)*lm$coefficients[1,2])`,`r lm$coefficients[1,1]+(qt(0.975,df=249)*lm$coefficients[1,2])`)

95% confidence intervals for $B_1$ is (`r lm$coefficients[2,1]-(qt(0.975,df=249)*lm$coefficients[2,2])`,`r lm$coefficients[2,1]+(qt(0.975,df=249)*lm$coefficients[2,2])`)

$R^2$ is `r lm$r.squared`, which means `r percent(lm$r.squared)` of variance in bodyfact could be explained by the variance of wrist circumference.

## Question 7
```{r}
new = data.frame(ABDOMEN=60)
(summary(lm(BODYFAT~ABDOMEN,data=data)))
```

Predicted body fat percentage of an individual with an abdominal circumference of 60 centimeters is `r percent(predict(lm(BODYFAT~ABDOMEN,data=data),new)/100)`

## Question 8
####(a) Read data and calculate weeks returns for spot and future.
```{r}
future <- read.csv('WeeklyCrudeOilSpotFutures.csv')
future$FutureRt <-  c(NA,(future$CRUDEFRONT[2:nrow(future)]/future$CRUDEFRONT[1:(nrow(future)-1)]-1)*100)
future$SpotRt <- c(NA,(future$CRUDESPOT[2:nrow(future)]/future$CRUDESPOT[1:(nrow(future)-1)]-1)*100)
```
####(b) 
Regress spot return on future return, the result is:
```{r}
(lm <- summary(lm(SpotRt~FutureRt,data=future)))
```
So in order to hedge the 50,000 barrels of crude oil position, I need to short sell `r 50000/1000*lm$coefficients[2,1]` future contracts

####(c)
Regress spot price on future price:
```{r}
(lm <- summary(lm(CRUDESPOT~CRUDEFRONT,data=future)))
```
For hypothesis $H_0: \beta = 1$, the p-value is `r (1-pt((lm$coefficients[2,1]-1)/lm$coefficients[2,2],df=nrow(future)-2))*2`, which is less than 0.001, so we can rejected the null hypothesis at 0.001 significance level, so for sure we can also reject the null hypothesis at 0.01 and 0.05 significance levels as well.

## Question 9
####(a)Read data and calculate returns
```{r}
CAPM1 <- read.csv('AAPLSP50020022006.csv')
CAPM2 <- read.csv("AAPLSP50020072011.csv")

CAPM1$AAPLrt <- c(NA,100*(CAPM1$AAPL[2:nrow(CAPM1)]/CAPM1$AAPL[1:(nrow(CAPM1)-1)]-1))
CAPM1$SPrt <- c(NA,100*(CAPM1$SP500[2:nrow(CAPM1)]/CAPM1$SP500[1:(nrow(CAPM1)-1)]-1))

CAPM2$AAPLrt <- c(NA,100*(CAPM2$AAPL[2:nrow(CAPM2)]/CAPM2$AAPL[1:(nrow(CAPM2)-1)]-1))
CAPM2$SPrt <- c(NA,100*(CAPM2$SP500[2:nrow(CAPM2)]/CAPM2$SP500[1:(nrow(CAPM2)-1)]-1))
```
Plot the histogram of the Apple returns.
```{r}
ggplot(CAPM1)+
    geom_histogram(aes(x=AAPLrt))

ggplot(CAPM2)+
    geom_histogram(aes(x=AAPLrt))
    
```

####(b)
Transforming annulized risk free rate to monthly risk free rate and subtract it from monthly returns of SP500 and Apple

For data set 1
```{r}
CAPM1$month_rf <- ((1+CAPM1$rf/100)^(1/12)-1)*100
CAPM1$AAPL_ExcessRt <- CAPM1$AAPLrt-CAPM1$month_rf
CAPM1$SP500_ExcessRt <- CAPM1$SPrt-CAPM1$month_rf
summary(lm(AAPL_ExcessRt~SP500_ExcessRt,data=CAPM1))
```
For data set 2
```{r}
CAPM2$month_rf <- ((1+CAPM2$rf/100)^(1/12)-1)*100
CAPM2$AAPL_ExcessRt <- CAPM2$AAPLrt-CAPM2$month_rf
CAPM2$SP500_ExcessRt <- CAPM2$SPrt-CAPM2$month_rf
summary(lm(AAPL_ExcessRt~SP500_ExcessRt,data=CAPM2))
```
The slopes are not similar, $\beta$ in the first period is 1.4616 at 0.001 significance level, which means Apples excess return would increase 1.4616%, on average, when market excess return is inceasing 1%.
$\beta$ is 1.2172 in the second time period at 0.001 significance level, which is lower than first period, which means Apples excess return would increase 1.2172%, on average, when market excess return is inceasing 1%.
The reason why $\beta$ decrease in the second horizon could be financial crisis during 2007-2008 that leads to sharply decrease for the whole market but Apples stock price performed relatively strong in that period but did not decrease so much as market did. Another reason could be as Apple's market value grows, it's stock price would likely be less volatile as before.

## Question 10
Read the data and check the regression result
```{r}
vote <- read.csv("VoteTotalsFloridaCountiesElection2000.csv")
summary(lm(vote$Bush~vote$Buchanan))
```
Draw the graph to find the outliers
```{r}
plot(vote$Bush~vote$Buchanan)
abline(lm(vote$Bush~vote$Buchanan))
```

From the picture above, we can observe there are two obvious outliers, one has the maximum votes for Buchanan while another has maximum votes for Bush, we then find out those two counties:
```{r}
filter(vote,Bush==max(Bush)|Buchanan==max(Buchanan)) %>% 
    select(County)
```
So the outliers are DADE and PALMBEACH. We then filter out those two counties to estimate the model.
```{r}
new_vote <- filter(vote,Bush!=max(Bush)&Buchanan!=max(Buchanan))
summary(lm(new_vote$Bush~new_vote$Buchanan))
```

Before deleting outliers, the slope is 79.39 while after deleting outliers, the slope is 195.683, which more than doubled, I can say in this model, $\hat\beta_1$ is sensitive to outliers. But in general, as the data set grows, the sensitivity to outliers should decrease.

## Question 11
Read the data
```{r}
Kelley <- read.csv("KelleyBlueBookData.csv")
```

Test $H_0:\rho=0$:
```{r}
cor <- cor.test(Kelley$Price,Kelley$Mileage,alternative = 'two.sided')
cor
```

So at 5% significance we can reject the null hopothesis that $H_0:\rho=0$.

Test $H_0:\beta_1=0$:

```{r}
summary(lm(Price~Mileage,data=Kelley))
```
Since the p-value for hypothesis test is 4.68e-05, We can reject the null hypothesis that $H_0:\beta_1=0$ at 0.001 significance level.

To test null hypothesis  $H_0$ : The regression relationship is not significant we can observe the result of F-statistics above, to p-value of F-statistics is 4.685e-05, so we can reject the null hypothesis at significance level at 0.001.

The R-squared is 0.02046, which means 2.046% variance of Price could be explained by the variance of Mileagge.

## Question 12
Read the data, and get the corresponding $R^2$ and Adjusted $R^2$.
```{r}
r <- read.csv('ExtraColumnsOfRandomData.csv')
r2 <- NULL
r2_adj <- NULL
num <- 1:27
for (i in num){
    r2[i] <- summary(lm(BODYFAT~.,data=r[,1:(i+1)]))$r.squared
    r2_adj[i] <- summary(lm(BODYFAT~.,data=r[,1:(i+1)]))$adj.r.squared
}
graph <- data.frame(num,r2,r2_adj) %>% 
    gather(r2_type,value,r2,r2_adj)
ggplot(graph)+
    geom_point(aes(x=num,y=value,color=r2_type))
```

From this graph we can see that, after taking the variable Hip as independent variable, both $R^2$ and Adjusted $R^2$ get increased a lot, with more random columns added into the model, $R^2$ keeps increasing while $R^2$ has a decreasing trend.